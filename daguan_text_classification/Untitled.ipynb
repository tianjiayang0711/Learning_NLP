{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of labels: 19\n",
      "Training set\n",
      "[    6.     514.     990.    1428.    1949.    2858.48 39759.  ]\n",
      "[   50.     842.    1618.    2346.    3201.    4720.96 55804.  ]\n",
      "Test set\n",
      "[    6.   516.   992.  1429.  1949.  2826. 19755.]\n",
      "[   50.   842.  1621.  2349.  3207.  4672. 31694.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_df = pd.read_csv('C:/dg/data/train_set.csv')\n",
    "    test_df = pd.read_csv('C:/dg/data/test_set.csv')\n",
    "\n",
    "    train_char = train_df['article'].values.tolist()\n",
    "    train_word = train_df['word_seg'].values.tolist()\n",
    "    train_label = train_df['class'].values\n",
    "    test_char = test_df['article'].values.tolist()\n",
    "    test_word = test_df['word_seg'].values.tolist()\n",
    "\n",
    "    num_label = len(set(train_label))\n",
    "    print(f'# of labels: {num_label}')\n",
    "\n",
    "    train_char_len = [len(chars.split()) for chars in train_char]\n",
    "    train_word_len = [len(words.split()) for words in train_word]\n",
    "    print('Training set')\n",
    "    print(np.percentile(train_word_len, [0, 50, 80, 90, 95, 98, 100]))\n",
    "    print(np.percentile(train_char_len, [0, 50, 80, 90, 95, 98, 100]))\n",
    "\n",
    "    test_char_len = [len(chars.split()) for chars in test_char]\n",
    "    test_word_len = [len(words.split()) for words in test_word]\n",
    "    print('Test set')\n",
    "    print(np.percentile(test_word_len, [0, 50, 80, 90, 95, 98, 100]))\n",
    "    print(np.percentile(test_char_len, [0, 50, 80, 90, 95, 98, 100]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_df = pd.read_csv('C:/dg/data/train_set.csv')\n",
    "    test_df = pd.read_csv('C:/dg/data/test_set.csv')\n",
    "\n",
    "    train_words = train_df['word_seg'].values.tolist()\n",
    "    test_words = test_df['word_seg'].values.tolist()\n",
    "    all_words = train_words + test_words\n",
    "\n",
    "    train_chars = train_df['article'].values.tolist()\n",
    "    test_chars = test_df['article'].values.tolist()\n",
    "    all_chars = train_chars + test_chars\n",
    "\n",
    "    with open('C:/dg/data/all_words.txt', 'w') as f:\n",
    "        for text in all_words:\n",
    "            f.write(f'{text}\\n')\n",
    "\n",
    "    with open('C:/dg/data/all_chars.txt', 'w') as f:\n",
    "        for text in all_chars:\n",
    "            f.write(f'{text}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "SEED = 2018\n",
    "np.random.seed(SEED)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_df = pd.read_csv('C:/dg/data/train_set.csv')\n",
    "    test_df = pd.read_csv('C:/dg/data/test_set.csv')\n",
    "\n",
    "    train_char = train_df['article'].values.tolist()\n",
    "    train_word = train_df['word_seg'].values.tolist()\n",
    "    train_label = train_df['class'].values - 1\n",
    "    test_char = test_df['article'].values.tolist()\n",
    "    test_word = test_df['word_seg'].values.tolist()\n",
    "\n",
    "    word_vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)\n",
    "    train_word_feat = word_vectorizer.fit_transform(train_word)\n",
    "    test_word_feat = word_vectorizer.transform(test_word)\n",
    "\n",
    "    svd = TruncatedSVD(n_components=100, n_iter=20, random_state=SEED)\n",
    "    train_svd_feat = svd.fit_transform(train_word_feat)\n",
    "    print('Training set transformed..')\n",
    "    with open('C:/dg/data/train_svd_feat.pkl', 'wb') as f:\n",
    "        pkl.dump(train_svd_feat, f)\n",
    "\n",
    "    test_svd_feat = svd.transform(test_word_feat)\n",
    "    print('Test set transformed..')\n",
    "    with open('C:/dg/data/test_svd_feat.pkl', 'wb') as f:\n",
    "        pkl.dump(test_svd_feat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    with open('C:/dg/data/train_svd_feat.pkl', 'rb') as f:\n",
    "        train_data = pkl.load(f)\n",
    "    with open('C:/dg/data/test_svd_feat.pkl', 'rb') as f:\n",
    "        test_data = pkl.load(f)\n",
    "    train_label = np.load('../../data/label.npy')\n",
    "\n",
    "    num_classes = len(set(train_label))\n",
    "\n",
    "    num_fold = 10\n",
    "    fold_len = train_data.shape[0] // num_fold\n",
    "\n",
    "    skf_indices = []\n",
    "    skf = StratifiedKFold(n_splits=num_fold, shuffle=True, random_state=2018)\n",
    "    for i, (train_idx, valid_idx) in enumerate(skf.split(np.ones(train_data.shape[0]), train_label)):\n",
    "        skf_indices.extend(valid_idx.tolist())\n",
    "\n",
    "    train_pred = np.zeros((train_data.shape[0], num_classes))\n",
    "    test_pred = np.zeros((test_data.shape[0], num_classes))\n",
    "\n",
    "    for fold in range(num_fold):\n",
    "\n",
    "        print(f'Processing fold {fold}...')\n",
    "\n",
    "        fold_start = fold * fold_len\n",
    "        fold_end = (fold + 1) * fold_len\n",
    "        if fold == num_fold - 1:\n",
    "            fold_end = len(skf_indices)\n",
    "\n",
    "        train_indices = skf_indices[:fold_start] + skf_indices[fold_end:]\n",
    "        test_indices = skf_indices[fold_start:fold_end]\n",
    "\n",
    "        train_x, test_x = train_data[train_indices], train_data[test_indices]\n",
    "        train_y = train_label[train_indices]\n",
    "\n",
    "        clf = LGBMClassifier(n_estimators=1000)\n",
    "        clf.fit(train_x, train_y)\n",
    "        pred = clf.predict_proba(test_x)\n",
    "        train_pred[test_indices] = pred\n",
    "        pred = clf.predict_proba(test_data)\n",
    "        test_pred += pred / num_fold\n",
    "\n",
    "    y_pred = np.argmax(train_pred, axis=1)\n",
    "    score = f1_score(train_label, y_pred, average='macro')\n",
    "    print(score)\n",
    "\n",
    "    np.save(f'../../oof_pred/_lgbm_svd_train_{score:.4f}', train_pred)\n",
    "    np.save(f'../../oof_pred/_lgbm_svd_test_{score:.4f}', test_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
