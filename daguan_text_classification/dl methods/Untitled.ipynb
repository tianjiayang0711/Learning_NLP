{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration is stopped.\n",
      "锘縤d         102277\n",
      "article     102277\n",
      "word_seg    102277\n",
      "class       102277\n",
      "dtype: int64\n",
      "Iteration is stopped.\n",
      "锘縤d         102277\n",
      "article     102277\n",
      "word_seg    102277\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import feather\n",
    "import pandas as pd\n",
    "\n",
    "def gen_csv_feather(path,path_new):\n",
    "    f = open(path)\n",
    "    reader = pd.read_csv(f, sep=',', iterator=True)\n",
    "    loop = True\n",
    "    chunkSize = 10000\n",
    "    chunks = []\n",
    "    while loop:\n",
    "        try:\n",
    "            chunk = reader.get_chunk(chunkSize)\n",
    "            chunks.append(chunk)\n",
    "        except StopIteration:\n",
    "            loop = False\n",
    "            print(\"Iteration is stopped.\")\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    print(df.count())\n",
    "    feather.write_dataframe(df,path_new)\n",
    "    \n",
    "gen_csv_feather(\"C:/dg/data/train_set.csv\",\"C:/dg/data/train_set.feather\")\n",
    "gen_csv_feather(\"C:/dg/data/test_set.csv\",\"C:/dg/data/test_set.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianjiayang\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\tianjiayang\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.utils.training_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-493e911a69dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.training_utils'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import feather\n",
    "import os\n",
    "import re\n",
    "import sys  \n",
    "import gc\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "import argparse\n",
    "parser=argparse.ArgumentParser()\n",
    "parser.add_argument(\"--gpu\",type=str)\n",
    "parser.add_argument(\"--column_name\",type=str)\n",
    "parser.add_argument(\"--word_seq_len\",type=int)\n",
    "parser.add_argument(\"--embedding_vector\",type=int)\n",
    "parser.add_argument(\"--num_words\",type=int)\n",
    "parser.add_argument(\"--model_name\",type=str)\n",
    "parser.add_argument(\"--batch_size\",type=int)\n",
    "parser.add_argument(\"--KFold\",type=int)\n",
    "parser.add_argument(\"--classification\",type=int)\n",
    "args=parser.parse_args()\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(\"../embedding\"):\n",
    "    os.mkdir(\"../embedding\")\n",
    "\n",
    "if not os.path.exists(\"../cache\"):\n",
    "    os.mkdir(\"../cache\")\n",
    "\n",
    "if not os.path.exists(\"../stacking\"):\n",
    "    os.mkdir(\"../stacking\")\n",
    "\n",
    "\n",
    "if not os.path.exists(\"../mid_result\"):\n",
    "    os.mkdir(\"../mid_result\")\n",
    "\n",
    "\n",
    "if not os.path.exists(\"../submission\"):\n",
    "    os.mkdir(\"../submission\")\n",
    "\n",
    "\n",
    "\n",
    "from TextModel import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "\n",
    "\n",
    "#导入数据\n",
    "train=feather.read_dataframe(\"../data/train_set.feather\")\n",
    "test=feather.read_dataframe(\"../data/test_set.feather\")\n",
    "\n",
    "\n",
    "\n",
    "#词向量\n",
    "def w2v_pad(df_train,df_test,col, maxlen_,victor_size):\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=args.num_words, lower=False,filters=\"\")\n",
    "    tokenizer.fit_on_texts(list(df_train[col].values)+list(df_test[col].values))\n",
    "\n",
    "    train_ = sequence.pad_sequences(tokenizer.texts_to_sequences(df_train[col].values), maxlen=maxlen_)\n",
    "    test_ = sequence.pad_sequences(tokenizer.texts_to_sequences(df_test[col].values), maxlen=maxlen_)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    \n",
    "    count = 0\n",
    "    nb_words = len(word_index)\n",
    "    print(nb_words)\n",
    "    all_data=pd.concat([df_train[col],df_test[col]])\n",
    "    file_name = '../embedding/' + 'Word2Vec_' + col  +\"_\"+ str(victor_size) + '.model'\n",
    "    if not os.path.exists(file_name):\n",
    "        model = Word2Vec([[word for word in document.split(' ')] for document in all_data.values],\n",
    "                         size=victor_size, window=5, iter=10, workers=11, seed=2018, min_count=2)\n",
    "        model.save(file_name)\n",
    "    else:\n",
    "        model = Word2Vec.load(file_name)\n",
    "    print(\"add word2vec finished....\")    \n",
    "\n",
    "\n",
    "    glove_model = {}\n",
    "    with open(\"../embedding/glove_vectors_word.txt\",encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove_model[word] = coefs\n",
    "    print(\"add glove finished....\")  \n",
    "                 \n",
    "    embedding_word2vec_matrix = np.zeros((nb_words + 1, victor_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = model[word] if word in model else None\n",
    "        if embedding_vector is not None:\n",
    "            count += 1\n",
    "            embedding_word2vec_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            unk_vec = np.random.random(victor_size) * 0.5\n",
    "            unk_vec = unk_vec - unk_vec.mean()\n",
    "            embedding_word2vec_matrix[i] = unk_vec\n",
    "\n",
    "\n",
    "    glove_count=0\n",
    "    embedding_glove_matrix = np.zeros((nb_words + 1, victor_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_glove_vector=glove_model[word] if word in glove_model else None\n",
    "        if embedding_glove_vector is not None:\n",
    "            glove_count += 1\n",
    "            embedding_glove_matrix[i] = embedding_glove_vector\n",
    "        else:\n",
    "            unk_vec = np.random.random(victor_size) * 0.5\n",
    "            unk_vec = unk_vec - unk_vec.mean()\n",
    "            embedding_glove_matrix[i] = unk_vec\n",
    "\n",
    "    embedding_matrix=np.concatenate((embedding_word2vec_matrix,embedding_glove_matrix),axis=1)\n",
    "    \n",
    "    print (embedding_matrix.shape, train_.shape, test_.shape, count * 1.0 / embedding_matrix.shape[0],glove_count*1.0/embedding_matrix.shape[0])\n",
    "    return train_, test_, word_index, embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_seq_len=args.word_seq_len\n",
    "victor_size=args.embedding_vector\n",
    "column_name=args.column_name\n",
    "train_, test_,word2idx, word_embedding = w2v_pad(train,test,column_name, word_seq_len,victor_size)\n",
    "\n",
    "\n",
    "\n",
    "def word_model_cv(my_opt):\n",
    "    #参数\n",
    "    lb = LabelEncoder()\n",
    "    train_label = lb.fit_transform(train['class'].values)\n",
    "    train_label = to_categorical(train_label)\n",
    "\n",
    "    if not os.path.exists(\"../cache/\"+my_opt):\n",
    "        os.mkdir(\"../cache/\"+my_opt)\n",
    "\n",
    "    #模型\n",
    "    my_opt=eval(my_opt)\n",
    "    name = str(my_opt.__name__)\n",
    "    kf = KFold(n_splits=args.KFold, shuffle=True, random_state=520).split(train_)\n",
    "    train_model_pred = np.zeros((train_.shape[0], args.classification))\n",
    "    test_model_pred = np.zeros((test_.shape[0], args.classification))\n",
    "\n",
    "    for i, (train_fold, test_fold) in enumerate(kf):\n",
    "        X_train, X_valid, = train_[train_fold, :], train_[test_fold, :]\n",
    "        y_train, y_valid = train_label[train_fold], train_label[test_fold]\n",
    "\n",
    "        print(i, 'fold')\n",
    "\n",
    "        the_path = '../cache/' + name +'/' +  name + \"_\" +args.column_name\n",
    "        model = my_opt(word_seq_len, word_embedding,args.classification)\n",
    "        early_stopping = EarlyStopping(monitor='val_acc', patience=6)\n",
    "        plateau = ReduceLROnPlateau(monitor=\"val_acc\", verbose=1, mode='max', factor=0.5, patience=3)\n",
    "        checkpoint = ModelCheckpoint(the_path + str(i) + '.hdf5', monitor='val_acc', verbose=2, save_best_only=True, mode='max',save_weights_only=True)\n",
    "        if not os.path.exists(the_path + str(i) + '.hdf5'):\n",
    "            print(\"error\")\n",
    "            model.fit(X_train, y_train,\n",
    "                      epochs=100,\n",
    "                      batch_size=args.batch_size,\n",
    "                      validation_data=(X_valid, y_valid),\n",
    "                      callbacks=[early_stopping, plateau, checkpoint],\n",
    "                      verbose=2)\n",
    "\n",
    "\n",
    "        model.load_weights(the_path + str(i) + '.hdf5')\n",
    "\n",
    "        print (name + \": valid's accuracy: %s\" % f1_score(lb.inverse_transform(np.argmax(y_valid, 1)), \n",
    "                                                          lb.inverse_transform(np.argmax(model.predict(X_valid), 1)).reshape(-1,1),\n",
    "                                                          average='micro'))\n",
    "    \n",
    "        train_model_pred[test_fold, :] =  model.predict(X_valid)\n",
    "        test_model_pred += model.predict(test_)\n",
    "        \n",
    "        del model; gc.collect()\n",
    "        K.clear_session()\n",
    "    #线下测试\n",
    "    print (name + \": offline test score: %s\" % f1_score(lb.inverse_transform(np.argmax(train_label, 1)), \n",
    "                                                  lb.inverse_transform(np.argmax(train_model_pred, 1)).reshape(-1,1),\n",
    "                                                  average='micro'))\n",
    "\n",
    "    #中间结果记录\n",
    "    mid_pred=test[['id']].copy()\n",
    "    mid_pred=pd.concat([mid_pred,pd.DataFrame(test_model_pred)],axis=1)\n",
    "\n",
    "    mid_pred.to_csv('../mid_result/{0}_KFold{1}_bs{2}_w2v{2}_len{3}_column_name{4}.csv'\n",
    "                                                                        .format(name,\n",
    "                                                                                args.KFold,\n",
    "                                                                                args.batch_size,\n",
    "                                                                                args.embedding_vector,\n",
    "                                                                                args.word_seq_len,\n",
    "                                                                                args.column_name\n",
    "                                                                                ),index=False)\n",
    "    \n",
    "    last_pred=test[['id']].copy()\n",
    "    last_pred['class']=lb.inverse_transform(np.argmax(test_model_pred, 1)).reshape(-1,1)\n",
    "    last_pred[['id',\"class\"]].to_csv('../submission/{0}_KFold{1}_bs{2}_w2v{2}_len{3}_column_name{4}.csv'\n",
    "                                                                                            .format(name,\n",
    "                                                                                                    args.KFold,\n",
    "                                                                                                    args.batch_size,\n",
    "                                                                                                    args.embedding_vector,\n",
    "                                                                                                    args.word_seq_len,\n",
    "                                                                                                    args.column_name\n",
    "                                                                                                    ),index=False)\n",
    "\n",
    "    #\n",
    "    test_model_pred /= args.KFold\n",
    "    np.savez(\"../stacking/\" + my_opt.__name__+ str(args.KFold) + '_' + args.column_name +'.npz', train=train_model_pred, test=test_model_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "word_model_cv(args.model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "def bi_gru_model(sent_length, embeddings_weight,class_num):\n",
    "    print(\"get_text_gru3\")\n",
    "    content = Input(shape=(sent_length,), dtype='int32')\n",
    "    embedding = Embedding(\n",
    "        name=\"word_embedding\",\n",
    "        input_dim=embeddings_weight.shape[0],\n",
    "        weights=[embeddings_weight],\n",
    "        output_dim=embeddings_weight.shape[1],\n",
    "        trainable=False)\n",
    "\n",
    "    x = SpatialDropout1D(0.2)(embedding(content))\n",
    "\n",
    "    x = Bidirectional(CuDNNGRU(200, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNGRU(200, return_sequences=True))(x)\n",
    "\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "\n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(1000)(conc))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(500)(x)))\n",
    "    output = Dense(class_num, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(inputs=content, outputs=output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
