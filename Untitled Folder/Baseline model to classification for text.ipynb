{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "df_technology = pd.read_csv(\"C:/NLP_project/data/technology_news.csv\", encoding='utf-8')\n",
    "df_technology = df_technology.dropna()\n",
    "\n",
    "df_car = pd.read_csv(\"C:/NLP_project/data/car_news.csv\", encoding='utf-8')\n",
    "df_car = df_car.dropna()\n",
    "\n",
    "df_entertainment = pd.read_csv(\"C:/NLP_project/data/entertainment_news.csv\", encoding='utf-8')\n",
    "df_entertainment = df_entertainment.dropna()\n",
    "\n",
    "df_military = pd.read_csv(\"C:/NLP_project/data/military_news.csv\", encoding='utf-8')\n",
    "df_military = df_military.dropna()\n",
    "\n",
    "df_sports = pd.read_csv(\"C:/NLP_project/data/sports_news.csv\", encoding='utf-8')\n",
    "df_sports = df_sports.dropna()\n",
    "\n",
    "technology = df_technology.content.values.tolist()[1000:21000]\n",
    "car = df_car.content.values.tolist()[1000:21000]\n",
    "entertainment = df_entertainment.content.values.tolist()[:20000]\n",
    "military = df_military.content.values.tolist()[:20000]\n",
    "sports = df_sports.content.values.tolist()[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=pd.read_csv(\"C:/NLP_project/data/stopwords.txt\",index_col=False,quoting=3,sep=\"\\t\",names=['stopword'], encoding='utf-8')\n",
    "stopwords=stopwords['stopword'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\TIANJI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.239 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(content_lines, sentences, category):\n",
    "    for line in content_lines:\n",
    "        try:\n",
    "            segs=jieba.lcut(line)\n",
    "            segs = filter(lambda x:len(x)>1, segs)\n",
    "            segs = filter(lambda x:x not in stopwords, segs)\n",
    "            sentences.append((\" \".join(segs), category))\n",
    "        except Exception:\n",
    "            print (line)\n",
    "            continue \n",
    "\n",
    "#生成训练数据\n",
    "sentences = []\n",
    "\n",
    "preprocess_text(technology, sentences, 'technology')\n",
    "preprocess_text(car, sentences, 'car')\n",
    "preprocess_text(entertainment, sentences, 'entertainment')\n",
    "preprocess_text(military, sentences, 'military')\n",
    "preprocess_text(sports, sentences, 'sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿森纳 近况 不佳 上周末 英超 焦点 客场 不敌 利物浦 本赛季 利物浦 联赛 双杀 英国 媒体 赛后 爆出 主帅 温格 智利 前锋 桑切斯 闹矛盾 sports\n",
      "腾讯 发挥 数据 能力 多年 对抗 网络 黑产 经验 联合 产业链 警方 多方 力量 推出 守护者 计划 平台 打击 危害 网络安全 犯罪行为 腾讯 守护者 计划 团队 协助 协助 深圳 警方 展开 大规模 打击 整治 骚扰 信息 专项 清查 涉及 骚扰 信息 违法犯罪 窝点 查获 个人信息 万条 配合 警方 破获 非法 贩卖 个人信息 案件 无锡 佳佳 广东 飓风 阻止 数亿 被盗 公民 信息 流向 黑市 technology\n",
      "2017 人工智能 产业 创业 创新 峰会 上史 文勇 不到 3% 智能 汽车 十年 这一 比例 提升 90% 基础 智能 设施 操作系统 实时 在线 安装 软件 拓展 功能 technology\n",
      "中新网 张家口 日电 王祖敏 冬奥会 成功 申办 中国 冰雪 项目 热度 升温 参加 国家级 专业 赛事 雪上运动 爱好者 非分之想 日于 张家口 太舞 滑雪场 喜玛尔图 中国 高山 滑雪 巡回赛 赛事 云集 业余选手 民间 高手 sports\n",
      "人员 不整 怨言 经历 假期 集合 队员 达到最佳 状态 对手 处于 联赛 中期 竞技状态 比赛 目标 国家队 寻找 球员 月份 集训 说不定 队员 入选 sports\n",
      "任性 再减 400 苏宁 首发 平台 优惠 空前 technology\n",
      "2010 日至 2012 期间 生产 进口 2010 2013 年款 力狮 系列 汽车 共计 3587 car\n",
      "解读 sports\n",
      "两部 纪录片 制片人 总导演 陈怡 科技 人类文明 驱动器 汽车 百年 梳理 汽车 第一个 百年 发展 历史 累计 收视 人群 2.4 汽车 百年 II 汽车 技术 未来 发展 畅想 希望 此基础 推出 电影版 观众 认可 car\n",
      "初中 学历证明 car\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(sentences)\n",
    "for sentence in sentences[:10]:\n",
    "    print (sentence[0], sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x, y = zip(*sentences)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65696"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    max_features=4000,  # keep the most common 4000 ngrams\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)#每一句话转换成一个 1*4000的稀疏向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8425498881227453"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加入抽取2-gram和3-gram的统计特征，比如可以把词库的量放大一点。\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    ngram_range=(1,4),  # use ngrams of size 1 and 2\n",
    "    max_features=20000,  # keep the most common 1000 ngrams\n",
    ")\n",
    "vec.fit(x_train)\n",
    "\n",
    "def get_features(x):\n",
    "    vec.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8798575277409927"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(x_train), y_train)\n",
    "classifier.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tianjiayang\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.880800400096738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "def stratifiedkfold_cv(x, y, clf_class, shuffle=True, n_folds=5, **kwargs):\n",
    "    stratifiedk_fold = StratifiedKFold(y, n_folds=n_folds, shuffle=shuffle)\n",
    "    y_pred = y[:]\n",
    "    for train_index, test_index in stratifiedk_fold:\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred \n",
    "\n",
    "NB = MultinomialNB\n",
    "print (precision_score(y, stratifiedkfold_cv(vec.transform(x),np.array(y),NB), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "svm.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(vec.transform(x_train), y_train)\n",
    "svm.score(vec.transform(x_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class TextClassifier():\n",
    "\n",
    "    def __init__(self, classifier=SVC(kernel='linear')):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=12000)\n",
    "\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.classifier.predict(self.features([x]))\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = TextClassifier()\n",
    "text_classifier.fit(x_train, y_train)\n",
    "print(text_classifier.predict('这 是 有史以来 最 大 的 一 次 军舰 演习'))\n",
    "print(text_classifier.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
